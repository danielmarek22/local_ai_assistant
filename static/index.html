<!DOCTYPE html>
<html>
<head>
    <title>Project Astra: Local AI</title>
    <style>
        /* --- GLOBAL STYLES --- */
        body { margin: 0; background-color: #111; color: white; overflow: hidden; font-family: 'Segoe UI', sans-serif; }
        #canvas-container { position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; z-index: 0; }

        /* --- CHAT INTERFACE --- */
        #chat-panel {
            position: absolute;
            top: 20px;
            left: 20px;
            width: 350px;
            height: calc(100vh - 40px);
            background: rgba(0, 0, 0, 0.6);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            border: 1px solid rgba(255, 255, 255, 0.1);
            display: flex;
            flex-direction: column;
            z-index: 10;
            box-shadow: 0 10px 30px rgba(0,0,0,0.5);
            transition: border-color 0.3s;
        }

        /* Status Header */
        #status-bar {
            padding: 15px;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            font-size: 14px;
            font-weight: bold;
            display: flex;
            align-items: center;
            gap: 10px;
            color: #ccc;
        }

        .status-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
            background-color: #555;
            box-shadow: 0 0 5px rgba(0,0,0,0.5);
            transition: background-color 0.3s, box-shadow 0.3s;
        }

        /* Status Colors */
        .status-idle .status-dot { background-color: #00ff99; box-shadow: 0 0 10px #00ff99; }
        .status-thinking .status-dot { background-color: #ffcc00; animation: pulse 1s infinite; }
        .status-searching .status-dot { background-color: #00ccff; animation: pulse 1s infinite; }
        .status-responding .status-dot { background-color: #00ff99; }

        @keyframes pulse { 0% { opacity: 1; } 50% { opacity: 0.5; } 100% { opacity: 1; } }

        #chat-history {
            flex: 1;
            padding: 20px;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
            gap: 15px;
            scrollbar-width: thin;
            scrollbar-color: #444 transparent;
        }

        /* Message Bubbles */
        .message {
            padding: 12px 16px;
            border-radius: 12px;
            max-width: 85%;
            word-wrap: break-word;
            font-size: 14px;
            line-height: 1.5;
            animation: fadeIn 0.3s ease;
        }

        @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }

        .message.user {
            align-self: flex-end;
            background: #00ff99;
            color: #000;
            border-bottom-right-radius: 2px;
            box-shadow: 0 2px 5px rgba(0,255,153,0.2);
        }

        .message.astra {
            align-self: flex-start;
            background: rgba(255, 255, 255, 0.1);
            color: #eee;
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-bottom-left-radius: 2px;
        }

        #input-area {
            padding: 15px;
            background: rgba(0, 0, 0, 0.3);
            border-top: 1px solid rgba(255, 255, 255, 0.1);
            display: flex;
            gap: 10px;
            border-bottom-left-radius: 15px;
            border-bottom-right-radius: 15px;
        }

        #user-input {
            flex: 1;
            padding: 12px;
            border-radius: 8px;
            border: 1px solid rgba(255,255,255,0.1);
            background: rgba(0, 0, 0, 0.5);
            color: white;
            outline: none;
            transition: border 0.2s;
        }
        #user-input:focus { border-color: #00ff99; }

        #send-btn {
            padding: 0 20px;
            border-radius: 8px;
            border: none;
            background: #00ff99;
            color: #000;
            font-weight: bold;
            cursor: pointer;
            transition: transform 0.1s;
        }
        #send-btn:active { transform: scale(0.95); }

    </style>
    
    <script type="importmap">
        {
            "imports": {
                "three": "https://cdn.jsdelivr.net/npm/three@0.169.0/build/three.module.js",
                "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.169.0/examples/jsm/",
                "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3.0.0/lib/three-vrm.module.js"
            }
        }
    </script>
</head>
<body>
    <div id="canvas-container"></div>
    
    <div id="chat-panel" class="status-idle">
        <div id="status-bar">
            <div class="status-dot"></div>
            <div id="status-text">Astra is Idle</div>
        </div>

        <div id="chat-history">
            <div class="message astra">System: Online. Ready.</div>
        </div>
        <div id="input-area">
            <input type="text" id="user-input" placeholder="Type a message..." autocomplete="off" />
            <button id="send-btn">Send</button>
        </div>
    </div>

    <script type="module">
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
        import { VRMLoaderPlugin, VRMUtils } from '@pixiv/three-vrm';

        // --- GLOBAL STATE ---
        let avatarState = "idle"; 
        const poseTarget = { neckX: 0, neckY: 0, spineY: 0 };
        
        // --- AUDIO SYSTEM ---
        const audioQueue = [];
        let isPlaying = false;
        let audioContext = null;
        let analyser = null;
        let dataArray = null;
        let audioSource = null;
        
        // Create a single HTML Audio Element to reuse
        const audioEl = new Audio();
        audioEl.crossOrigin = "anonymous";

        // Setup Audio Context (Must be triggered by user interaction first)
        function initAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256; // Defines resolution of analysis
                dataArray = new Uint8Array(analyser.frequencyBinCount);
                
                // Connect Audio Element -> Analyser -> Speakers
                const track = audioContext.createMediaElementSource(audioEl);
                track.connect(analyser);
                analyser.connect(audioContext.destination);
                console.log("Audio Context Initialized");
            } else if (audioContext.state === 'suspended') {
                audioContext.resume();
            }
        }

        // --- 1. SETUP 3D SCENE ---
        const renderer = new THREE.WebGLRenderer({ alpha: true, antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.setPixelRatio(window.devicePixelRatio);
        document.getElementById('canvas-container').appendChild(renderer.domElement);

        const scene = new THREE.Scene();
        const camera = new THREE.PerspectiveCamera(30.0, window.innerWidth / window.innerHeight, 0.1, 20.0);
        camera.position.set(1.0, 1.0, 4.0); 

        const controls = new OrbitControls(camera, renderer.domElement);
        controls.enableDamping = true;
        controls.dampingFactor = 0.05;
        controls.target.set(0.0, 0.9, 0.0);
        controls.update();

        const light = new THREE.DirectionalLight(0xffffff, 1.0);
        light.position.set(1.0, 1.0, 1.0).normalize();
        scene.add(light);
        const ambientLight = new THREE.AmbientLight(0xffffff, 0.4);
        scene.add(ambientLight);

        // --- 2. LOAD VRM MODEL ---
        let currentVrm = null;
        const loader = new GLTFLoader();
        loader.register((parser) => new VRMLoaderPlugin(parser));

        loader.load(
            '/static/avatar.vrm', 
            (gltf) => {
                const vrm = gltf.userData.vrm;
                VRMUtils.removeUnnecessaryVertices(gltf.scene);
                VRMUtils.removeUnnecessaryJoints(gltf.scene);
                currentVrm = vrm;
                scene.add(vrm.scene);
                vrm.scene.rotation.y = Math.PI; 
                vrm.humanoid.getNormalizedBoneNode('leftUpperArm').rotation.z = 1.2; 
                vrm.humanoid.getNormalizedBoneNode('rightUpperArm').rotation.z = -1.2;
            },
            (progress) => {},
            (error) => console.error(error)
        );

        // --- 3. ANIMATION LOOP ---
        const clock = new THREE.Clock();

        function animate() {
            requestAnimationFrame(animate);
            const deltaTime = clock.getDelta();
            const time = clock.elapsedTime;
            controls.update(); 

            if (currentVrm) {
                currentVrm.update(deltaTime);
                
                // --- LIP SYNC LOGIC ---
                // If audio is playing and we have an analyser, get volume
                if (analyser && !audioEl.paused) {
                    analyser.getByteFrequencyData(dataArray);
                    
                    // Calculate average volume (RMS-ish)
                    let sum = 0;
                    for (let i = 0; i < dataArray.length; i++) {
                        sum += dataArray[i];
                    }
                    const average = sum / dataArray.length;
                    
                    // Map volume (0-255) to mouth open (0.0-1.0)
                    // Sensitivity: divide by 80 to make it open easier
                    let openValue = Math.min(1.0, average / 60); 
                    
                    // Smooth the mouth movement
                    const currentMouth = currentVrm.expressionManager.getValue('aa');
                    const smoothedMouth = THREE.MathUtils.lerp(currentMouth, openValue, 0.3);
                    
                    currentVrm.expressionManager.setValue('aa', smoothedMouth);
                } else {
                    // Close mouth if silent
                    currentVrm.expressionManager.setValue('aa', 0);
                }

                // --- STATE ANIMATIONS ---
                const neck = currentVrm.humanoid.getNormalizedBoneNode('neck');
                const chest = currentVrm.humanoid.getNormalizedBoneNode('chest');

                // Standard Breathing
                const breath = Math.sin(time * 1.0);
                chest.rotation.x = breath * 0.03;

                // Pose Smoothing
                neck.rotation.x = THREE.MathUtils.lerp(neck.rotation.x, poseTarget.neckX, 0.05);
                neck.rotation.y = THREE.MathUtils.lerp(neck.rotation.y, poseTarget.neckY, 0.05);
                currentVrm.humanoid.getNormalizedBoneNode('spine').rotation.y =
                    THREE.MathUtils.lerp(
                        currentVrm.humanoid.getNormalizedBoneNode('spine').rotation.y,
                        poseTarget.spineY,
                        0.05
                    );

                // Idle Sway
                const sway = Math.sin(time * 0.5) * 0.02;
                neck.rotation.y += sway;
                
                // Blinking
                const blinkChance = (avatarState === "thinking" || avatarState === "searching") ? 0.002 : 0.005;
                if (Math.random() < blinkChance) blink();
            }
            renderer.render(scene, camera);
        }
        animate();

        function blink() {
            if (!currentVrm) return;
            currentVrm.expressionManager.setValue('blink', 1.0);
            setTimeout(() => currentVrm.expressionManager.setValue('blink', 0.0), 150);
        }

        // --- 4. AUDIO QUEUE PLAYBACK ---
        function playNextAudio() {
            if (isPlaying || audioQueue.length === 0) return;

            isPlaying = true;
            const audioUrl = audioQueue.shift(); // Get first URL

            // Load and Play
            audioEl.src = audioUrl;
            
            // Try/Catch for AutoPlay policy
            audioEl.play().catch(e => {
                console.error("Audio play failed (interaction needed?):", e);
                isPlaying = false;
            });

            audioEl.onended = () => {
                isPlaying = false;
                playNextAudio(); // Play next in queue
            };
        }

        // --- 5. UI HELPERS ---
        const chatHistory = document.getElementById('chat-history');
        const userInput = document.getElementById('user-input');
        const chatPanel = document.getElementById('chat-panel');
        const statusText = document.getElementById('status-text');
        let currentAiMessageDiv = null; 

        function updateState(state) {
            avatarState = state;
            chatPanel.classList.remove('status-idle', 'status-thinking', 'status-searching', 'status-responding');
            chatPanel.classList.add(`status-${state}`);

            if (state === 'idle') {
                statusText.innerText = "Astra is Idle";
                poseTarget.neckX = 0; poseTarget.neckY = 0; poseTarget.spineY = 0;
            }
            if (state === 'thinking') {
                statusText.innerText = "Astra is Thinking...";
                poseTarget.neckX = -0.25; poseTarget.neckY = 0.35; poseTarget.spineY = 0.05;
            }
            if (state === 'searching') {
                statusText.innerText = "Searching Knowledge Base...";
                poseTarget.neckX = -0.15; poseTarget.neckY = -0.4; poseTarget.spineY = -0.05;
            }
            if (state === 'responding') {
                statusText.innerText = "Astra is Responding";
                poseTarget.neckX = 0.05; poseTarget.neckY = 0; poseTarget.spineY = 0;
            }
        }

        function appendMessage(sender, text) {
            const msgDiv = document.createElement('div');
            msgDiv.classList.add('message');
            msgDiv.classList.add(sender === 'user' ? 'user' : 'astra');
            msgDiv.innerText = text;
            chatHistory.appendChild(msgDiv);
            chatHistory.scrollTop = chatHistory.scrollHeight;
            return msgDiv; 
        }

        // --- 6. WEBSOCKET CONNECTION ---
        try {
            const ws = new WebSocket("ws://localhost:8000/ws");
            
            ws.onopen = () => { updateState('idle'); };

            ws.onmessage = function(event) {
                const data = JSON.parse(event.data);
                
                // 1. STATE EVENT
                if (data.type === "assistant_state") {
                    updateState(data.state);
                    if (data.state === "responding" && !currentAiMessageDiv) {
                        currentAiMessageDiv = appendMessage('astra', '');
                    }
                } 
                
                // 2. CHUNK EVENT (Text)
                else if (data.type === "assistant_chunk") {
                    if(avatarState !== 'responding') updateState('responding');
                    if (!currentAiMessageDiv) currentAiMessageDiv = appendMessage('astra', '');
                    
                    currentAiMessageDiv.innerText += data.content;
                    chatHistory.scrollTop = chatHistory.scrollHeight;
                }
                
                // 3. AUDIO EVENT (New!)
                else if (data.type === "assistant_audio") {
                    // Add URL to queue and attempt playback
                    // Ensure the URL matches your local path structure
                    audioQueue.push(data.url);
                    playNextAudio();
                }

                // 4. END EVENT
                else if (data.type === "assistant_end") {
                    if (currentAiMessageDiv) currentAiMessageDiv.innerText = data.content;
                    currentAiMessageDiv = null; 
                    updateState('idle'); 
                }
            };

            function sendMessage() {
                const text = userInput.value.trim();
                if (!text) return;

                // INITIALIZE AUDIO ON FIRST INTERACTION
                initAudioContext();
                
                if (ws.readyState === WebSocket.OPEN) {
                    appendMessage('user', text);
                    ws.send(text);
                    userInput.value = "";
                }
            }

            document.getElementById('send-btn').addEventListener('click', sendMessage);
            userInput.addEventListener('keypress', (e) => {
                if (e.key === 'Enter') sendMessage();
            });

        } catch (e) { console.log(e); }
    </script>
</body>
</html>